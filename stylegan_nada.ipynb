{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "stylegan_nada.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPadsLl1RZGico01OYHHz47",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/gist/rinongal/1e7f043750485445cd5b870a864c4043/stylegan_nada.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYsd0_RFXb04"
      },
      "source": [
        "# Welcome to StyleGAN-NADA: Zero-shot Non-Adversarial Domain Adaptation of GANs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTHeOO8qFw_e"
      },
      "source": [
        "# Step 1: Setup required libraries and models. \n",
        "This may take a few minutes.\n",
        "\n",
        "You may optionally enable downloads with pydrive in order to authenticate and avoid drive download limits when fetching pre-trained ReStyle and StyleGAN2 models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ph3R7lbl_arQ",
        "cellView": "form"
      },
      "source": [
        "#@title Setup\n",
        "%tensorflow_version 1.x\n",
        "\n",
        "import os\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "pretrained_model_dir = os.path.join(\"/content\", \"models\")\n",
        "os.makedirs(pretrained_model_dir, exist_ok=True)\n",
        "\n",
        "restyle_dir = os.path.join(\"/content\", \"restyle\")\n",
        "stylegan_ada_dir = os.path.join(\"/content\", \"stylegan_ada\")\n",
        "stylegan_nada_dir = os.path.join(\"/content\", \"stylegan_nada\")\n",
        "\n",
        "output_dir = os.path.join(\"/content\", \"output\")\n",
        "\n",
        "output_model_dir = os.path.join(output_dir, \"models\")\n",
        "output_image_dir = os.path.join(output_dir, \"images\")\n",
        "\n",
        "download_with_pydrive = True #@param {type:\"boolean\"}    \n",
        "    \n",
        "class Downloader(object):\n",
        "    def __init__(self, use_pydrive):\n",
        "        self.use_pydrive = use_pydrive\n",
        "\n",
        "        if self.use_pydrive:\n",
        "            self.authenticate()\n",
        "        \n",
        "    def authenticate(self):\n",
        "        auth.authenticate_user()\n",
        "        gauth = GoogleAuth()\n",
        "        gauth.credentials = GoogleCredentials.get_application_default()\n",
        "        self.drive = GoogleDrive(gauth)\n",
        "    \n",
        "    def download_file(self, file_id, file_dst):\n",
        "        if self.use_pydrive:\n",
        "            downloaded = self.drive.CreateFile({'id':file_id})\n",
        "            downloaded.FetchMetadata(fetch_all=True)\n",
        "            downloaded.GetContentFile(file_dst)\n",
        "        else:\n",
        "            !gdown --id $file_id -O $file_dst\n",
        "\n",
        "downloader = Downloader(download_with_pydrive)\n",
        "\n",
        "# install requirements\n",
        "!pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html \n",
        "\n",
        "!git clone https://github.com/yuval-alaluf/restyle-encoder.git $restyle_dir\n",
        "\n",
        "downloader.download_file(\"1sw6I2lRIB0MpuJkpc8F5BJiSZrc0hjfE\", os.path.join(pretrained_model_dir, \"restyle_psp_ffhq_encode.pt\"))\n",
        "downloader.download_file(\"1e2oXVeBPXMQoUoC_4TNwAWpOPpSEhE_e\", os.path.join(pretrained_model_dir, \"restyle_e4e_ffhq_encode.pt\"))\n",
        "\n",
        "!wget https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n",
        "!sudo unzip ninja-linux.zip -d /usr/local/bin/\n",
        "!sudo update-alternatives --install /usr/bin/ninja ninja /usr/local/bin/ninja 1 --force\n",
        "\n",
        "!pip install ftfy regex tqdm \n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "\n",
        "!git clone https://github.com/NVlabs/stylegan2-ada/ $stylegan_ada_dir\n",
        "!git clone https://github.com/rinongal/stylegan-nada.git $stylegan_nada_dir\n",
        "\n",
        "from argparse import Namespace\n",
        "\n",
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "sys.path.append(restyle_dir)\n",
        "sys.path.append(stylegan_nada_dir)\n",
        "sys.path.append(os.path.join(stylegan_nada_dir, \"ZSSGAN\"))\n",
        "\n",
        "from restyle.utils.common import tensor2im\n",
        "from restyle.models.psp import pSp\n",
        "from restyle.models.e4e import e4e\n",
        "\n",
        "device = 'cuda'\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSL166pfGRWF"
      },
      "source": [
        "# Step 2: Choose a model type.\n",
        "Model will be downloaded and converted to a pytorch compatible version.\n",
        "\n",
        "Re-runs of the cell with the same model will re-use the previously downloaded version. Feel free to experiment and come back to previous models :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4ATNsC1k28g",
        "cellView": "form"
      },
      "source": [
        "source_model_type = 'ffhq' #@param['ffhq', 'cat', 'dog', 'church', 'horse']\n",
        "\n",
        "source_model_download_path = {\"ffhq\":   \"https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/pretrained/ffhq.pkl\",\n",
        "                              \"cat\":    \"https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/pretrained/afhqcat.pkl\",\n",
        "                              \"dog\":    \"https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/pretrained/afhqdog.pkl\",\n",
        "                              \"church\": \"1iDo5cUgbwsJEt2uwfgDy_iPlaT-lLZmi\",\n",
        "                              \"car\":    \"1i-39ztut-VdUVUiFuUrwdsItR--HF81w\",\n",
        "                              \"horse\":  \"1irwWI291DolZhnQeW-ZyNWqZBjlWyJUn\"}\n",
        "\n",
        "model_names = {\"ffhq\":   \"ffhq.pkl\",\n",
        "               \"cat\":    \"afhqcat.pkl\",\n",
        "               \"dog\":    \"afhqdog.pkl\",\n",
        "               \"church\": \"stylegan2-church-config-f.pkl\",\n",
        "               \"car\":    \"stylegan2-car-config-f.pkl\",\n",
        "               \"horse\":  \"stylegan2-horse-config-f.pkl\"}\n",
        "\n",
        "download_string = source_model_download_path[source_model_type]\n",
        "file_name = model_names[source_model_type]\n",
        "pt_file_name = file_name.split(\".\")[0] + \".pt\"\n",
        "\n",
        "dataset_sizes = {\n",
        "    \"ffhq\":   1024,\n",
        "    \"cat\":    512,\n",
        "    \"dog\":    512,\n",
        "    \"church\": 256,\n",
        "    \"horse\":  256\n",
        "}\n",
        "\n",
        "if not os.path.isfile(os.path.join(pretrained_model_dir, file_name)):\n",
        "    print(\"Downloading chosen model...\")\n",
        "\n",
        "    if download_string.endswith(\".pkl\"):\n",
        "        !wget $download_string -O $pretrained_model_dir/$file_name\n",
        "    else:\n",
        "        downloader.download_file(download_string, os.path.join(pretrained_model_dir, file_name))\n",
        "        \n",
        "if not os.path.isfile(os.path.join(pretrained_model_dir, pt_file_name)):\n",
        "    print(\"Converting sg2 model. This may take a few minutes...\")\n",
        "    \n",
        "    tf_path = next(filter(lambda x: \"tensorflow\" in x, sys.path), None)\n",
        "    py_path = tf_path + f\":{stylegan_nada_dir}/ZSSGAN\"\n",
        "    convert_script = os.path.join(stylegan_nada_dir, \"convert_weight.py\")\n",
        "    !PYTHONPATH=$py_path python $convert_script --repo $stylegan_ada_dir --gen $pretrained_model_dir/$file_name"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAri8ULOG2VE"
      },
      "source": [
        "# Step 3: Train the model.\n",
        "Describe your source and target class. These describe the direction of change you're trying to apply (e.g. \"photo\" to \"sketch\", \"dog\" to \"the joker\" or \"dog\" to \"avocado dog\").\n",
        "\n",
        "For changes that do not require drastic shape modifications, we reccomend lambda_direction = 1.0, lambda_patch = 0.0, lambda_global = 0.0.\n",
        "\n",
        "More drastic changes may require turning on the global loss (and / or modifying the number of iterations).\n",
        "\n",
        "\n",
        "As a rule of thumb:\n",
        "- Style and minor domain changes ('photo' -> 'sketch') require ~200-400 iterations.\n",
        "- Identity changes ('person' -> 'taylor swift') require ~150-200 iterations.\n",
        "- Simple in-domain changes ('face' -> 'smiling face') may require as few as 50.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "> 26/06 update - added an option to train only the middle network layers. This will keep background / shapes more consistent but limit the extent of changes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YrtPb7KF8m-",
        "cellView": "form"
      },
      "source": [
        "from ZSSGAN.model.ZSSGAN import ZSSGAN\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "\n",
        "from tqdm import notebook\n",
        "\n",
        "from ZSSGAN.utils.file_utils import save_images\n",
        "\n",
        "from IPython.display import display\n",
        "\n",
        "source_class = \"photo\" #@param {\"type\": \"string\"}\n",
        "target_class = \"sketch\" #@param {\"type\": \"string\"}\n",
        "\n",
        "lambda_direction = 1.0 #@param {type:\"number\"}\n",
        "lambda_patch = 0.0 #@param {type:\"number\"}\n",
        "lambda_global = 0.0 #@param {type:\"number\"}\n",
        "\n",
        "training_iterations = 301 #@param {type: \"integer\"}\n",
        "output_interval     = 50 #@param {type: \"integer\"}\n",
        "\n",
        "train_partial_layers = False #@param {type:\"boolean\"}\n",
        "phase = \"texture\" if train_partial_layers else None\n",
        "\n",
        "training_args = {\n",
        "            \"size\": dataset_sizes[source_model_type],\n",
        "            \"batch\": 2,\n",
        "            \"n_sample\": 4,\n",
        "            \"output_dir\": output_dir,\n",
        "            \"lr\": 0.002,\n",
        "            \"frozen_gen_ckpt\": os.path.join(pretrained_model_dir, pt_file_name),\n",
        "            \"train_gen_ckpt\": os.path.join(pretrained_model_dir, pt_file_name),\n",
        "            \"iter\": training_iterations,\n",
        "            \"source_class\": source_class,\n",
        "            \"target_class\": target_class,\n",
        "            \"lambda_direction\": lambda_direction,\n",
        "            \"lambda_patch\": lambda_patch,\n",
        "            \"lambda_global\": lambda_global,\n",
        "            \"phase\": phase,\n",
        "            \"sample_truncation\": 0.7\n",
        "}\n",
        "\n",
        "args = Namespace(**training_args)\n",
        "\n",
        "net = ZSSGAN(args)\n",
        "\n",
        "g_reg_ratio = 4 / 5\n",
        "\n",
        "g_optim = torch.optim.Adam(\n",
        "    net.generator_trainable.parameters(),\n",
        "    lr=args.lr * g_reg_ratio,\n",
        "    betas=(0 ** g_reg_ratio, 0.99 ** g_reg_ratio),\n",
        ")\n",
        "\n",
        "# Set up output directories.\n",
        "sample_dir = os.path.join(args.output_dir, \"sample\")\n",
        "ckpt_dir   = os.path.join(args.output_dir, \"checkpoint\")\n",
        "\n",
        "os.makedirs(sample_dir, exist_ok=True)\n",
        "os.makedirs(ckpt_dir, exist_ok=True)\n",
        "\n",
        "seed = 3 #@param {\"type\": \"integer\"}\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "# Training loop\n",
        "fixed_z = torch.randn(args.n_sample, 512, device=device)\n",
        "\n",
        "for i in notebook.tqdm(range(args.iter)):\n",
        "\n",
        "    sample_z = torch.randn(args.batch, 512, device=device)\n",
        "\n",
        "    [sampled_src, sampled_dst], [cycle_dst, cycle_src], clip_loss, cycle_loss = net([sample_z])\n",
        "\n",
        "    net.zero_grad()\n",
        "    clip_loss.backward()\n",
        "\n",
        "    g_optim.step()\n",
        "\n",
        "    if i % output_interval == 0:\n",
        "        with torch.no_grad():\n",
        "            [sampled_src, sampled_dst], [cycle_dst, cycle_src], clip_loss, cycle_loss = net([fixed_z], truncation=args.sample_truncation)\n",
        "\n",
        "            grid_rows = int(args.n_sample ** 0.5)\n",
        "\n",
        "            save_images(sampled_dst, sample_dir, \"dst\", grid_rows, i)\n",
        "\n",
        "            display(Image.open(os.path.join(sample_dir, f\"dst_{str(i).zfill(6)}.png\")).resize((512, 512)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZZk6yZQvxGY"
      },
      "source": [
        "# Step 4: Generate samples with the new model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "dLinyTgev5Qk"
      },
      "source": [
        "truncation = 0.7 #@param {type:\"slider\", min:0, max:1, step:0.05}\n",
        "\n",
        "samples = 9\n",
        "\n",
        "with torch.no_grad():\n",
        "    sample_z = torch.randn(samples, 512, device=device)\n",
        "\n",
        "    [sampled_src, sampled_dst], [cycle_dst, cycle_src], clip_loss, cycle_loss = net([sample_z], truncation=truncation)\n",
        "\n",
        "    grid_rows = int(samples ** 0.5)\n",
        "\n",
        "    save_images(sampled_dst, sample_dir, \"sampled\", grid_rows, 0)\n",
        "\n",
        "    display(Image.open(os.path.join(sample_dir, f\"sampled_{str(0).zfill(6)}.png\")).resize((768, 768)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4hVHBrlGxzo"
      },
      "source": [
        "## Editing a real image with Re-Style inversion (currently only FFHQ inversion is supported):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jv4h9WZ2gZio"
      },
      "source": [
        "Step 1: Set up the Re-Style model.\n",
        "\n",
        "Choose the pSp model for better reconstructions. \n",
        "We found that for some extreme modifications (typically those which require 500+ training iterations), e4e may perform better."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcSXQ3AbPrqh",
        "cellView": "form"
      },
      "source": [
        "encoder_type = 'psp' #@param['psp', 'e4e']\n",
        "\n",
        "restyle_experiment_args = {\n",
        "    \"model_path\": os.path.join(pretrained_model_dir, f\"restyle_{encoder_type}_ffhq_encode.pt\"),\n",
        "    \"transform\": transforms.Compose([\n",
        "        transforms.Resize((256, 256)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
        "}\n",
        "\n",
        "model_path = restyle_experiment_args['model_path']\n",
        "ckpt = torch.load(model_path, map_location='cpu')\n",
        "\n",
        "opts = ckpt['opts']\n",
        "\n",
        "opts['checkpoint_path'] = model_path\n",
        "opts = Namespace(**opts)\n",
        "\n",
        "restyle_net = (pSp if encoder_type == 'psp' else e4e)(opts)\n",
        "\n",
        "restyle_net.eval()\n",
        "restyle_net.cuda()\n",
        "print('Model successfully loaded!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfB-jTnZgn0D"
      },
      "source": [
        "Step 2: Align and invert an image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tMd5WBvE0Ol",
        "cellView": "form"
      },
      "source": [
        "def run_alignment(image_path):\n",
        "    import dlib\n",
        "    from scripts.align_faces_parallel import align_face\n",
        "    if not os.path.exists(\"shape_predictor_68_face_landmarks.dat\"):\n",
        "        print('Downloading files for aligning face image...')\n",
        "        os.system('wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2')\n",
        "        os.system('bzip2 -dk shape_predictor_68_face_landmarks.dat.bz2')\n",
        "        print('Done.')\n",
        "    predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
        "    aligned_image = align_face(filepath=image_path, predictor=predictor) \n",
        "    print(\"Aligned image has shape: {}\".format(aligned_image.size))\n",
        "    return aligned_image \n",
        "\n",
        "image_path = \"/content/ariana.jpg\" #@param {'type': 'string'}\n",
        "original_image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "input_image = run_alignment(image_path)\n",
        "\n",
        "display(input_image)\n",
        "\n",
        "img_transforms = restyle_experiment_args['transform']\n",
        "transformed_image = img_transforms(input_image)\n",
        "\n",
        "def get_avg_image(net):\n",
        "    avg_image = net(net.latent_avg.unsqueeze(0),\n",
        "                    input_code=True,\n",
        "                    randomize_noise=False,\n",
        "                    return_latents=False,\n",
        "                    average_code=True)[0]\n",
        "    avg_image = avg_image.to('cuda').float().detach()\n",
        "    return avg_image\n",
        "\n",
        "opts.n_iters_per_batch = 5\n",
        "opts.resize_outputs = False  # generate outputs at full resolution\n",
        "\n",
        "from restyle.utils.inference_utils import run_on_batch\n",
        "\n",
        "with torch.no_grad():\n",
        "    avg_image = get_avg_image(restyle_net)\n",
        "    result_batch, result_latents = run_on_batch(transformed_image.unsqueeze(0).cuda(), restyle_net, opts, avg_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOiIZcJUgsQS"
      },
      "source": [
        "Step 3: Convert the image to the new domain"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5JqEOMnEA_m",
        "cellView": "form"
      },
      "source": [
        "#@title Convert inverted image.\n",
        "inverted_latent = torch.Tensor(result_latents[0][4]).cuda().unsqueeze(0).unsqueeze(1)\n",
        "\n",
        "with torch.no_grad():\n",
        "    [sampled_src, sampled_dst] = net(inverted_latent, input_is_latent=True)[0]\n",
        "    \n",
        "    joined_img = torch.cat([sampled_src, sampled_dst], dim=0)\n",
        "    save_images(joined_img, sample_dir, \"joined\", 2, 0)\n",
        "    display(Image.open(os.path.join(sample_dir, f\"joined_{str(0).zfill(6)}.png\")).resize((512, 256)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
